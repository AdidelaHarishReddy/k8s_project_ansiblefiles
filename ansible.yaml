---
- name: Add Git repo to ArgoCD and create application
  hosts: argo_servers
  become: true
  gather_facts: yes

  vars:
    argocd_server: "argocd.example.com"  # Replace with actual ArgoCD server
    playbook_dir: "/home/ubuntu/ansible/k8s_project_ansiblefiles"
    argocd_username: "admin"
    argocd_password: "your-password"     # Store this securely (e.g., in Ansible Vault)
    git_repo_url: "https://github.com/AdidelaHarishReddy/project_k8s_yaml.git"
    git_repo_name: "my_project_yaml_repo"
    app_name: "my-app"
    app_namespace: "project"
    app_path: "./"
    kube_dest_server: "https://kubernetes.default.svc"
    aws_access_key: "{{ lookup('env', 'AWS_ACCESS_KEY_ID') | default('') }}"
    aws_secret_key: "{{ lookup('env', 'AWS_SECRET_ACCESS_KEY') | default('') }}"

  tasks:

    - name: Install ArgoCD CLI
      ansible.builtin.shell: |
        curl -sSL -o /usr/local/bin/argocd https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64
        chmod +x /usr/local/bin/argocd
      args:
        creates: /usr/local/bin/argocd  # Don't re-download if already installed


    - name: getting the host name
      ansible.builtin.shell: |
        curl -s "https://checkip.amazonaws.com"
      register: hostname_master

    - name: getting the argocd nodeport
      ansible.builtin.shell: |
        kubectl get svc -n argocd | awk '{print $5}' | tail -2 | head -1 | grep -o '443:[0-9]*' | cut -d ':' -f2
        # kubectl get svc argocd-server -n argocd -o jsonpath='{.spec.ports[?(@.port==443)].nodePort}' 
      register: nodeport
      environment:
         KUBECONFIG: /home/ubuntu/.kube/config

    - name: Debug nodeport
      ansible.builtin.debug:
        msg: "NodePort: {{ nodeport.stdout }}"
      ignore_errors: yes

    - name: getting argocd password
      ansible.builtin.shell: |
        kubectl get secrets -n argocd argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d | xargs -I{} echo "{}"
      register: password_raw
      environment:
         KUBECONFIG: /home/ubuntu/.kube/config

    - name: Debug password
      ansible.builtin.debug:
        msg: "password: {{ password_raw.stdout }}"
      ignore_errors: yes

    - name: Set nodePort fact
      set_fact:
        nodePort: "{{ nodeport.stdout | trim }}"

    - name: Wait until ArgoCD is reachable
      wait_for:
        host: "{{ hostname_master.stdout | trim }}"
        port: "{{ nodePort | int }}"
        timeout: 120

    - name: Login to ArgoCD
      ansible.builtin.shell: |
        argocd login "{{ hostname_master.stdout | trim }}:{{ nodeport.stdout | trim }}" --username {{ argocd_username }} --password {{ password_raw.stdout }} --insecure
      environment:
        ARGOCD_INSECURE: "true"  # Optional: avoid cert checks (not recommended for prod)

    - name: Add Git repository to ArgoCD
      ansible.builtin.shell: |
        argocd repo add {{ git_repo_url }} --name {{ git_repo_name }}
      register: repo_add_result
      failed_when: "'already exists' not in repo_add_result.stderr and repo_add_result.rc != 0"

    - name: Create Kubernetes namespace
      ansible.builtin.shell: |
        kubectl create namespace {{ app_namespace }}
      register: ns_result
      failed_when: "'already exists' not in ns_result.stderr and ns_result.rc != 0"
      environment:
         KUBECONFIG: /home/ubuntu/.kube/config


    - name: Create ArgoCD Application
      ansible.builtin.shell: |
        argocd app create {{ app_name }} \
          --repo {{ git_repo_url }} \
          --path {{ app_path }} \
          --dest-server {{ kube_dest_server }} \
          --dest-namespace {{ app_namespace }} \
          --project default \
          --sync-policy automated \
          --auto-prune \
          --self-heal
      register: app_create_result
      failed_when: "'already exists' not in app_create_result.stderr and app_create_result.rc != 0"

    - name: Sync ArgoCD Application (optional)
      ansible.builtin.shell: |
        argocd app sync {{ app_name }} --force
      register: sync_result
      until: sync_result.rc == 0
      retries: 5
      delay: 10

    - name: creating aws key_id& access_key
      ansible.builtin.shell: |
          kubectl create secret generic aws-secret --namespace kube-system --from-literal key_id= {{ aws_access_key }} --from-literal access_key= {{ aws_secret_key }} 
      environment:
         KUBECONFIG: /home/ubuntu/.kube/config
      register: aws_credentials      
      failed_when: "'already exist' not in aws_credentials.stderr and app_create_result.rc != 0"     
    
    - name: installing aws_csi
      ansible.builtin.shell: |
          kubectl apply -k "github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-1.45"
      environment:
         KUBECONFIG: /home/ubuntu/.kube/config

    - name: installing helm
      ansible.builtin.shell: |
          curl -s https://raw.githubusercontent.com/AdidelaHarishReddy/installations/refs/heads/main/HELM-ubuntu | bash         
      environment:
         KUBECONFIG: /home/ubuntu/.kube/config
      delay: 5     
       
    - name: adding promethius repo
      ansible.builtin.shell: |
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo update
      environment:
        KUBECONFIG: /home/ubuntu/.kube/config
        HOME: /home/ubuntu
      become: false
           # failed_when: "'already exists' not in app_create_result.stderr and app_create_result.rc != 0"

    - name: creating the new namespace for monitoring
      ansible.builtin.shell: |
          kubectl create ns monitoring
      environment:
         KUBECONFIG: /home/ubuntu/.kube/config
      failed_when: "'already exists' not in app_create_result.stderr and app_create_result.rc != 0"      

    - name: Create custom values file for kube-prometheus-stack
      become: false
      ansible.builtin.copy:
        dest: ./custom_kube_prometheus_stack.yml
        content: |
          alertmanager:
            enabled: true
            alertmanagerSpec:
              # Selects Alertmanager configuration based on these labels.
              alertmanagerConfigSelector:
                matchLabels:
                  release: monitoring
              replicas: 2
              alertmanagerConfigMatcherStrategy:
                type: None
                  
    - name: installing prometheus-communinty charts
      ansible.builtin.shell: |         
        helm install monitoring prometheus-community/kube-prometheus-stack -n monitoring -f ./custom_kube_prometheus_stack.yml
        sleep 10
      environment:
        KUBECONFIG: /home/ubuntu/.kube/config
      become: false     
      failed_when: "'already exists' not in app_create_result.stderr and app_create_result.rc != 0"     
          
    - name: installing INGRESS NGINX CONTROLLER charts
      ansible.builtin.shell: |
          helm install my-release oci://ghcr.io/nginx/charts/nginx-ingress \
            --version 2.3.1 \
            --namespace ingress-nginx \
            --create-namespace \
            --set controller.kind=daemonset \
            --set controller.service.type=NodePort \
            --set controller.hostPort.enabled=true
          sleep 2
      environment:
          KUBECONFIG: /home/ubuntu/.kube/config
      become: false
      failed_when: "'already exists' not in app_create_result.stderr and app_create_result.rc != 0"      

    - name: Copy ingress file from local to remote
      become: false
      ansible.builtin.copy:
        src: "{{ playbook_dir }}/monitoring_ingress.yaml"         # Relative to playbook directory on controller
        dest: /home/ubuntu/
        owner: ubuntu
        group: ubuntu
        mode: '0777'

    - name: applying INGRESS RESOURCE file for monitoring namespace
      ansible.builtin.shell: | 
        kubectl apply -f monitoring_ingress.yaml -n monitoring
      environment:
          KUBECONFIG: /home/ubuntu/.kube/config
      become: false
      failed_when: "'already exists' not in app_create_result.stderr and app_create_result.rc != 0"  

#     installing the iam polocies and roles for create aplication load balancer
    - name: downloding the aws_loadbalancer_json_polocy
      ansible.builtin.shell: |
         curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json
      environment:
        HOME: /home/ubuntu
      become: false
      register: download_result
      failed_when: download_result.rc != 0

    # - name: debugging the aws_loadbalancer_json_polocy
    #   ansible.builtin.debug:
    #     msg: "attaching profiledata: {{ download_result }}"
    #   ignore_errors: yes    

    - name: installing aws cli
      ansible.builtin.shell: |
        curl -s https://raw.githubusercontent.com/AdidelaHarishReddy/installations/refs/heads/main/install_aws_cli | bash
      environment:
        HOME: /home/ubuntu
        DEBIAN_FRONTEND: noninteractive  # Suppresses apt/dpkg prompts globally
      args:
        creates: /usr/local/bin/aws    
      become: false
      register: install_result
      failed_when: install_result.rc != 0 

    # - name: debugging the installing aws cli
    #   ansible.builtin.debug:
    #     msg: "attaching profiledata: {{ install_result }}"
    #   ignore_errors: yes    

    - name: aws credentials configure
      ansible.builtin.shell: |
          aws configure set aws_access_key_id "{{ aws_access_key }}"
          aws configure set aws_secret_access_key "{{ aws_secret_key }}"
          aws configure set default.region ap-south-1
          aws configure set output json
      environment:
        HOME: /home/ubuntu
      become: false
      register: config_result
      failed_when: config_result.rc != 0

    - name: debugging the aws credentials configure
      ansible.builtin.debug:
        msg: "attaching profiledata: {{ config_result }}"
      ignore_errors: yes    

    - name: creating the polocy for aws_alb from iam_polocy.json
      ansible.builtin.shell: |
        aws iam create-policy \
          --policy-name AWSLoadBalancerControllerIAMPolicy \
          --policy-document file://iam-policy.json
      environment:
        HOME: /home/ubuntu
      become: false
      register: policy_result
      failed_when: "'EntityAlreadyExists' not in policy_result.stderr and policy_result.rc != 0"

    - name: debugging the polocy for aws_alb from iam_polocy.json
      ansible.builtin.debug:
        msg: "attaching profiledata: {{ policy_result }}"
      ignore_errors: yes    
    
    - name: Create an IAM Role for EC2
      ansible.builtin.shell: |
        aws iam create-role \
          --role-name EC2NodeRoleForALB \
          --assume-role-policy-document '{
            "Version": "2012-10-17",
            "Statement": [
              {
                "Effect": "Allow",
                "Principal": { "Service": "ec2.amazonaws.com" },
                "Action": "sts:AssumeRole"
              }
            ]
          }'
      environment:
        HOME: /home/ubuntu
      become: false
      register: role_result
      failed_when: "'EntityAlreadyExists' not in role_result.stderr and role_result.rc != 0"
    
    - name: debugging the IAM Role for EC2
      ansible.builtin.debug:
        msg: "attaching profiledata: {{ role_result }}"
      ignore_errors: yes

    - name: create an instance profile
      ansible.builtin.shell: |
        aws iam create-instance-profile --instance-profile-name EC2NodeRoleForALB
      environment:
        HOME: /home/ubuntu
      become: false
      register: profile_result
      failed_when: "'EntityAlreadyExists' not in profile_result.stderr and profile_result.rc != 0"
    
    - name: debugging the instance profile
      ansible.builtin.debug:
        msg: "Inastance profile: {{ profile_result }}"
      ignore_errors: yes

    - name: attaching role to instance profile
      ansible.builtin.shell: |
        aws iam add-role-to-instance-profile \
          --instance-profile-name EC2NodeRoleForALB \
          --role-name EC2NodeRoleForALB
      environment:
        HOME: /home/ubuntu
      become: false
      register: attach_role
      failed_when:
        - attach_role.rc != 0
        - "'EntityAlreadyExists' not in attach_role.stderr"
        - "'LimitExceeded' not in attach_role.stderr"
      changed_when: "'EntityAlreadyExists' not in attach_role.stderr"

    - name: debugging the ataching role to instance profile
      ansible.builtin.debug:
        msg: "Attaching role to instance profile: {{ attach_role }}"
      ignore_errors: yes    
     
    - name: Attach the ALB Policy to the Role
      ansible.builtin.shell: |
        aws iam attach-role-policy \
          --role-name EC2NodeRoleForALB \
          --policy-arn arn:aws:iam::546771244881:policy/AWSLoadBalancerControllerIAMPolicy
      environment:
        HOME: /home/ubuntu
      become: false
      register: attach_policy_result
      failed_when: attach_policy_result.rc != 0

    - name: debugging the Attach the ALB Policy to the Role
      ansible.builtin.debug:
        msg: "Attach the ALB Policy to the Role: {{ attach_policy_result }}"
      ignore_errors: yes

    - name: Attach AmazonEBSCSIDriverPolicy to node role
      ansible.builtin.shell: |
        aws iam attach-role-policy \
          --role-name EC2NodeRoleForALB \
          --policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy
      environment:
        HOME: /home/ubuntu
      become: false
      register: ebs_policy_attach
      failed_when: ebs_policy_attach.rc != 0

    - name: debugging the Attach the ALB Policy to the Role
      ansible.builtin.debug:
        msg: "Attach the ALB Policy to the Role: {{ ebs_policy_attach }}"
      ignore_errors: yes
    
#    - name: Find EC2 worker instance IDs and Attach IAM Instance Profile to each EC2 instance
#      ansible.builtin.shell: >
#        aws ec2 describe-instances --region ap-south-1
#        --filters "Name=tag:Name,Values=*worker*"
#        --query "Reservations[].Instances[].InstanceId"
#        --output text |
#        awk -F ' ' '{print $1}{print $2}' |
#        xargs -I {} aws ec2 associate-iam-instance-profile
#        --instance-id {} --iam-instance-profile Name=EC2NodeRoleForALB
#        --region ap-south-1
#      register: attach_profile
#      failed_when: attach_profile.rc != 0
#      changed_when: false

#    - name: debugging the attaching the profile to ec2 instances
#      ansible.builtin.debug:
#        msg: "attaching profiledata: {{ attach_profile }}"    
#      ignore_errors: yes

    - name: Get worker EC2 instance IDs
      amazon.aws.ec2_instance_info:
        region: ap-south-1
        filters:
          "tag:Name": "*worker*"
          instance-state-name: running
      become: false   #for boto3 modules we need to remove root previlages because it will check for aws credentials at root/.aws/credentials but its avilable at home/ubuntu/.aws/credentials   
      register: worker_instances

    - name: Set worker instance IDs
      ansible.builtin.set_fact:
        worker_instance_ids: >-
          {{ worker_instances.instances | map(attribute='instance_id') | list }}

    - name: Debug worker instance IDs
      ansible.builtin.debug:
        var: worker_instance_ids

    - name: Attach IAM role to EC2 instances
      amazon.aws.ec2_instance:
        instance_ids: "{{ worker_instance_ids }}"
        iam_instance_profile: EC2NodeRoleForALB
        region: ap-south-1
      become: false    
      
    - name: creating the ALB as well as ingress resource
      ansible.builtin.include_role:
        name: alb
      register: alb_result    





# - name: for installing the iam polocies and roles for create aplication load balancer
#   hosts: data_servers
#   become: true
#   gather_facts: yes 

#   vars:
#     aws_access_key: "{{ lookup('env', 'AWS_ACCESS_KEY_ID') | default('') }}"
#     aws_secret_key: "{{ lookup('env', 'AWS_SECRET_ACCESS_KEY') | default('') }}"
#     iam_profile_name: "EC2NodeRoleForALB"
#     tag_filter: "*worker*"
#     aws_region: "ap-south-1"

#   tasks:

#     - name: Validate AWS credentials
#       ansible.builtin.command: aws sts get-caller-identity
#       environment:
#         AWS_ACCESS_KEY_ID: "{{ aws_access_key | trim }}"
#         AWS_SECRET_ACCESS_KEY: "{{ aws_secret_key | trim }}"
#         AWS_DEFAULT_REGION: ap-south-1
#       register: aws_identity
#       changed_when: false
#       failed_when: "'InvalidClientTokenId' in aws_identity.stderr"

#     - name: Discover EC2 instance IDs based on tag filter
#       ansible.builtin.command: >
#         aws ec2 describe-instances
#         --region {{ aws_region }}
#         --filters 'Name=tag:Name,Values={{ tag_filter }}'
#         --query 'Reservations[*].Instances[*].InstanceId'
#         --output text
#       environment:
#         HOME: /home/ubuntu
#         AWS_ACCESS_KEY_ID: "{{ aws_access_key | trim }}"
#         AWS_SECRET_ACCESS_KEY: "{{ aws_secret_key | trim }}"
#         AWS_DEFAULT_REGION: "{{ aws_region | trim }}"
#       become: false
#       register: ec2_instances
#       changed_when: false
    
#     - name: Debug - Show discovered instance IDs
#       ansible.builtin.debug:
#         msg: "Found instances: {{ ec2_instances.stdout_lines }}"
    
#     - name: Attach IAM Instance Profile to each EC2 instance
#       ansible.builtin.command: >
#         aws ec2 associate-iam-instance-profile
#         --instance-id {{ item }}
#         --iam-instance-profile Name={{ iam_profile_name }}
#         --region {{ aws_region }}
#       environment:
#         HOME: /home/ubuntu
#         AWS_ACCESS_KEY_ID: "{{ aws_access_key | trim }}"
#         AWS_SECRET_ACCESS_KEY: "{{ aws_secret_key | trim }}"
#         AWS_DEFAULT_REGION: "{{ aws_region | trim }}"
#       become: false
#       loop: "{{ ec2_instances.stdout_lines }}"
#       register: attach_results
#       changed_when: attach_results.rc == 0
#       failed_when: "'InvalidInstanceID' in attach_results.stderr or ('ResourceInUseException' not in attach_results.stderr and attach_results.rc != 0)" 

# - name: creating the ALB as well as ingress resource through ingress resource yml
#   hosts: argo_servers
#   become: true
#   gather_facts: yes

#   vars:
#     aws_access_key: "{{ lookup('env', 'AWS_ACCESS_KEY_ID') | default('') }}"
#     aws_secret_key: "{{ lookup('env', 'AWS_SECRET_ACCESS_KEY') | default('') }}"

#   tasks:
